# Learning Morality

## Outline

* Deep Learning (AI) state of the art
    * How intelligent **are** the machines?
        * Ng 1-second rule: Machines can be taught to make any 1-second decision that a human can make
        * YOLO can track and label objects in video with latency and accuracy comparable to humans
        * [Baidu Deep Speech 2](https://www.technologyreview.com/s/544651/baidus-deep-learning-system-rivals-people-at-speech-recognition/) and [MS LACE](https://arxiv.org/abs/1610.05256): speech-to-text accuracy parity with humans
        * [Google Waymo](https://www.google.com/selfdrivingcar/files/reports/report-0515.pdf): self-driving car [at-fault accident rate] 10x better than [human drivers](https://en.wikipedia.org/wiki/Aviation_safety#Transport_comparisons)
        * [Medical and transportation](http://files.quizsnack.com/iframe/embed.html?hash=q715o9n1) personnel likely to be displaced in near future (10-20 years)
    * How is Deep Learning different from Machine Learning?
        * ML wins for most practical business problems
        * DL models are less explainable and tunable
        * Ng 1-second rule: DL can make most decisions faster and more accurately than a human
        * DL requires more data than ML (because models are more complex)
        * Wayz and Google Maps 
* The control problem (AI morality motivation)
    * What's the problem?
        * Elon Musk et. al. [open letter]() to UN
        * Superintelligence will be motivated by self-interest if predecessors are taught self-interest
        * [Militaries are weaonizing AI](https://futureoflife.org/open-letter-autonomous-weapons/)
        * [Hackers harnessing AI to find vulnerable people](http://gizmodo.com/hackers-have-already-started-to-weaponize-artificial-in-1797688425)
    * Historical examples
        * Biological examples: mind-altering [viruses](http://www.rightdiagnosis.com/t/toxoplasmosis/stats-country.htm) and [bacteria](https://www.scientificamerican.com/article/mental-health-may-depend-on-creatures-in-the-gut/)
        * Human: mind-altering corporations (advertisement, pharmaceuticals, psycho-active products)
    * When will it affect my life?
        * Superintelligence has arrived (Corporations are cyborg societies)
        * Your life and your morality are already being influenced by Facebook, Google, etc
        * University students are being taught by AI [chatbots](https://qz.com/1065818/ai-university/)
        * AI can [detect your sexual orientation](https://www.gsb.stanford.edu/sites/gsb/files/publication-pdf/wang_kosinski.pdf)
    * Is prosocial behavior *adaptive*?
        * Social evolutionary dynamics predictions
        * [Simulations of evolution](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5241693/) suggest microbiome is to **blame** for altruism
        * The origin of "spite" (maladaptive for individual, winning strategy for community)
    * Philosophy
        * Tragedy of the commons
        * Prisoner's Dilemma
        * Trolley Problem
    * Game Theory & Social Evolutionary Dynamics
        * Nash Equilibrium in business and AI
        * There are no zero-sum games to "win" in the real world
        * The Price equation
        * Sexual selection in Machine Learning
        * DL weight-sharing 
* A solution?
    * Don't control, outcompete
    * Don't manipulate, collaborate
    * Teach the value of "other" and diversity
        * Other companies
        * Other AI
        * Other people
        * Other species
        * Other perspectives (ensemble methods -> swarm-sourcing)
    * Prioritize customer, community, 5-year investors over public equity investors
    * Radical transparency (open science, open data, open source, open discussions)
    * Ruthless altruism (destroy antisocial competition)
    * Build business intelligence systems that "win" by helping your community
    * Never shirk your moral decision responsibility, no matter what the lawyers tell you
    * Teach morality to your coworkers **and** their machines
    * Incorporate prosocial skill into [AI IQ tests](https://techxplore.com/news/2017-10-chinese-google-alphago-smarter-siri.html)

## Unabridged Abstract

Big thinkers like Musk (SpaceX), Hassabis (Deep Mind), Suleyman (Deep Mind), and Benjio (Element AI) have put their names on the line by forecasting the emminent emmergence of superintelligence, and the urgency of preventing the weaponization of AI. Other famous AI architects, like Andrew Ng, have compared the worry about the emergence of evil superintelligence to worrying about the overpopulation of Mars. So, to frame this talk I will briefly enumerate the arguments for and against AI morality research & development.

You'll be left to draw your own conclusions about the urgency of AI morality research and the pending emergence of superintelligence as I reframe the debate to address more tangible and immediate decisions that you make every day. I will motivate you to more deeply understand machine learning by showing you how it is making moral decisions on your behalf. Software developers are teaching machines to make moral decisions every day, often without even knowing it. I will provide examples of real world machine learning systems that have been trained by Data Scientists to make amoral choices. These engineers and business decision makers are unknowingly laying the foundation for antisocial machine intelligence as they grow in complexity.

Tesla engineers are deciding which inhabitants of this planet are worth braking for. Google developers decide what news will help you vote in an informed way. Facebook engineers teach machine learning systems which genders, sexual orientation, and races of people you should meet. Twitter's algorithms decide which ideas of the collective consciousness are newsworthy. It's your responsibility as decision makers to elevate these important decisions from the Agile scrum of developers up to the board room. Your executives and investors must take responsibility for the morality of the AI systems you are releasing into the wild. These are autonomous systems that may live on in software and deep learning connections long after you company is gone. Just as Monsanto and Dupont executives calculate the risks of releasing GMO and chemical products into the agricultural ecosystem that feeds the world, you must consider the risks of releasing your bots into the world.

After motivating a deeper understanding of deep learning I'll provide concrete examples of how Total Good builds prosocial moral decision-making into the feedback loops of its algorithms. And this approach will be shown as a potential solution to the AI "Control Problem" that relies only on the mathematics of social evolutionary dynamics. Antisocial AI cannot be regulated away, it must be outcompeted. Prosocial AI will help your business thrive.

## Bio

Hobson is the cofounder and President of Total Good, a nonprofit dedicate to propagating prosocial machine intelligence. Hobson has worked as the lead Data Scientist in a variety of industries, from consumer electronics to aerospace and robotics. Hobson has designed and built state-of-the art machine learning pipelines for laboratories at Northrop Grumman, Intel, and Sharp Corp., as well as startups like Talentpair, Building Energy, and Authority Labs. Hobson is the lead author of "Natural Language Processing In Action" to be published February 2018. Hobson thrives on sharing what he learns, contributing to open source projects, open science data, and teaching at schools like Springboard and Hack Oregon.

## Prosocial Organism Examples

- gut bacteria that don't kill us
- 15 octopi in "Octopolis"
- ants (species and colonties sometimes live in cooperation with others)
- symbiotic cross-species relationships
    - clown fish and anemonies
    - coral reef polups and algae
    - lichen, fungi, algae, and bacteria (viruses?)

## Antisocial Organism Examples

Some seemingly antisocial organisms and behaviors may be adaptive for an ecosystem as a whole or at least for a community of organisms.

- ant brain virus that causes them to climb grass and wait to be eaten
- toxo plasmosis in humans increases risk-taking & risk-taking propelled homosapien ahead of neanderthal
- parasite epidemics (reducing population densities to sustainable levels, increasing species diversity)
- spite and revenge enforce social norms while being maladaptive for the individual

## Sexual Reproduction

- leads to complex behavior
- appreciation for and love for an "other" (gender)
- display and signaling and communication
- complex social structures?

## Industry Trends

Poll by Andrew Ng at Spark Conf 2016 ignored many industries like law, construction, professional services, and blue collar services like hairdressing and fast food restaurant work.

Respondents: 574

Which industry is most likely to be transformed by AI in the near term (this is really just a poll of the industry mix of attendees at Spark Conf, industries using Spark. 

1. Healthcare: 35%
2. Transportation: 21%
3. Media/Entertainment: 10%
4. Manufacturing: 10%
5. Retail: 7%
6. Agriculture: 6%
7. Education: 6%
8. Energy: 5%

## Autonomous Cars

Self-driving cars will have to differentiate between selfish self-driving cars and prosocial self-driving cars.
Auto manufacturers may differentiate themselves by being the prosocial one (low emission Prius, zero-emission Tesla) or selfish (gas-guzzling american SUV manufacturers).
Eventually the prosocial, forward-thinking, big-picture vision companies and engineers always win.
So regulation cannot solve this riddle is not the answer, big-thinking, altruistic design is!

Must make safety-critical predictions like predicting the gesture of a construction worker making eye contact and beconing forward toards him, vs holding a hand up to signal stop.[1]

### Moral Route-Finding

In some cases self-serving route-optimization algorithms serve the common good.  
When Waze helps you avoid traffic [it helps reduce conjestion for everyone](https://www.researchgate.net/publication/261083823_Adaptive_Traffic_Optimization)

In other cases route-optimization can harm the common welfare:

* routing traffic through residential neighborhoods
    * endangers children and pedestrians
    * increases pollution/asthma in residential neighborhoods
    * increases noise pollution in quiet parts of town
* routing car traffic onto bike streets
    * endangers cyclists, skaters, etc
    * increases pollutants inhaled by cyclists, etc
* routing car traffic near schools, hospitals, nursing homes, fire stations, etc

How much do you think weighs the public good vs your **thumbs-up** frequency?  
What is the appropriate weight?  

This is a moral decision, and should be optimized with as much thought as customer satisfaction or revenue.

## Slides

<!SLIDE>

.callout.quote The higher the buildings, the lower the morals. 

> The higher the buildings,
> the lower the morals. 
> -- Noel Coward

<!SLIDE bullets incremental transition=fade>

# Drone Crashing

* Amazon delivery mission
* Battery overheating
* 4 lb package

<!SLIDE transition=fade>

# You have 3 seconds... 

1. highway: slow moving cars
2. side street: parked cars, pedestrians
3. park: people and pets
4. private rooftop
5. private yard/garden

<!SLIDE transition=fade>

# Dynamic programming problem

- partial info
- weakly adversarial
- stochastic
- chaotic


<!SLIDE transition=fade>

# Naive Approach

- safe ditch location DB
  - continuous ditch planning
  - motion detection of ditch sites
- health monitor
  - range estimator
  - life estimator

e.g. Safe2Ditch

~~~SECTION:notes~~~

allowance for adaptive morals?
parking lots aren't stationary active 
flags on rooftops
trees have leaves and branches that sway
sunbathers?
cars don't move at intersection

~~~ENDSECTION~~~


~~~SECTION:references~~~

## References

### Inequality Arbitrage and AI Morality

- Quant Trading by Karen Rubin: [video](http://pyvideo.org/pycon-us-2016/karen-rubin-building-a-quantitative-trading-strategy-to-beat-the-sp500-pycon-2016.html), [slides](https://speakerdeck.com/pycon2016/karen-rubin-building-a-quantitative-trading-strategy-to-beat-the-s-and-p500)
- Bertrand and Mullainathan "...More Employable than Lakisha and Jamal": [NBER draft paper](http://www.nber.org/papers/w9873), [2004 Indiana State paper](http://www2.econ.iastate.edu/classes/econ321/Orazem/bertrand_emily.pdf)
- [Wang & Kosinski](https://osf.io/fk3xr/download)
- [machine learning isn't the arbiter of truth and morality](https://www.ted.com/talks/cathy_o_neil_the_era_of_blind_faith_in_big_data_must_end)
- [moral decsions about the technology of self-driving cars](https://www.ted.com/talks/iyad_rahwan_what_moral_decisions_should_driverless_cars_make)

### Amoral AI

- [hackers use machine learning to find soft points](http://gizmodo.com/hackers-have-already-started-to-weaponize-artificial-in-1797688425)
- [autonomous weapons](https://futureoflife.org/open-letter-autonomous-weapons/)

### AI Trends

- [Andrew Ng Poll Results](http://files.quizsnack.com/iframe/embed.html?hash=q715o9n1)
- [Deep Speech 2](https://www.technologyreview.com/s/544651/baidus-deep-learning-system-rivals-people-at-speech-recognition/)

### Driverless Cars

- [Patrick Lin: Ethical dilemma of self-driving cars](https://www.youtube.com/watch?v=ixIoDYVfKA0): Moral, ethical decisions that machines must make, that humans don't have to.
- [Iyad Rahwan: Moral decisions driverless cars should make](https://www.ted.com/talks/iyad_rahwan_what_moral_decisions_should_driverless_cars_make): Webapp to build a moral framework for self-driving cars (great training set!) 
- [Edmond Awad, Sohan Dsouza, Iyad Rahwan, Azim Shariff, Jean-François Bonnefon: Moral Machine](https://www.media.mit.edu/research/groups/10005/moral-machine)
- [The Verge: Trolley Problem](https://www.theverge.com/2016/6/23/12010476/social-dilemma-autonomous-vehicles-car-moral-machine-trolley-problem)
- [moral machine for android](http://androidforfree1.download/games/who-would-you-kill-moral-machine)
- [Edmond Awad: Collective Intelligence, Group Argumentation, and logial ethics decisionmaking](http://web.media.mit.edu/~awad/)
- [a realistic self-driving car ethical delimma](https://youtu.be/ixIoDYVfKA0)

### Advertising

Advertising and Sales is inherently anti-social (capitalistic).
If machines are good at manipulating us and are motivated by survival instincts and self-interest, they will quickly learn to influence us to help them survive by replicating themselves and expanding their power/influence in the world.

- [Andrew Ng: AI is focused on predicting Ads we'll click on, selling us stuff](https://www.youtube.com/watch?v=21EiKfQYZXc)


### Non-virtuous Cycle of Hype

[Ng isn't worried about Superintelligence](https://youtu.be/21EiKfQYZXc?t=39m8s)
Each year, technological breakthroughs in machine intelligence and computing make it ever more likely that Ng will be proven wrong within our lifetime.  But he is right to focus policymakers on the more immediate problem of job displacement.

Ng feels it's a misallocation of resources (his friends are not involved in anti-evil AI research)?

Ng is more concerned about job displacement by AI

Andrew Ng's [lecture to Stanford MBA students](https://youtu.be/21EiKfQYZXc?t=41m36s) advises them to "sleep easy", worrying about evil AI is like worrying about overpopulation of Mars

Andrew Ng dismisses the Trolly Problem as irrelevant. But [that assessment](https://youtu.be/21EiKfQYZXc) ignores the fact that the trolly problem is an analogy for a whole class of real-world ethical dilemmas that we make decisions about every day. And every self-driving car decision is a balance between safety of the driver and others in the environment or among various environmental elements including both animals and people. For example the tailgating distance for a self-driving car behind a human car has several effects that must be balanced with more than purely rational self-interest "in mind."

* safety of the self-driving car occupants
* safety of the human-driven car
* safety of cars behind the self-driving car
* speed of travel for the self-driving car
* speed of travel for the cars nearby
* safety of animals and pedestrians that might be ahead
* fuel efficiency gain or loss from drafting
* environmental impact from emissions and noise

Of course these ethical dilemmas don't have as obvious or immediate effects as the alegory of the Trolley Problem, but because it will be a constant concern of every self-driving car in traffic, the decisions will have a huge impact.

[Andrew Ng "sleep easy", worrying about evil AI is like worrying about overpopulation of Mars

Ng seems to dismiss several reasonable arguments by smart people (see Surviving AI) without any analysis or thought.
Seems like a false analogy. A more appropriate analogy might be worrying about the seeding of Mars with viral/bacteria life from Earth that destroys existing life on Mars or makes it uninhabitable.

~~~ENDSECTION~~~


References:

* [AI IQ tests](https://techxplore.com/news/2017-10-chinese-google-alphago-smarter-siri.html)