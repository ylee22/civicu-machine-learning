{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Classification Using NLP\n",
    "\n",
    "Prepared by Chris Gian for Hack Oregon's Week \n",
    "Sources:\n",
    "- Based mostly on: [Lab 10 of Harvard's CS109](https://github.com/cs109/2015lab10) class. \n",
    "- Adapted for use with sklearn way of text processing: [Working With Text Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "- Hobson Lane's book: [NLP in Action](https://www.manning.com/books/natural-language-processing-in-action)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Setting up \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Modules used for transforming \"rotten tomatoes\" reviews into a structured format.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Read Data\n",
    "critics = pd.read_csv('resources/critics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 27631\n",
      "Number of columns: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows: %i\" % critics.shape[0])\n",
    "print(\"Number of columns: %i\" % critics.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27631, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14792, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Owen Gleiberman</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Entertainment Weekly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-09-07</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            critic  fresh    imdb           publication  \\\n",
       "0  Owen Gleiberman  fresh  114709  Entertainment Weekly   \n",
       "1      Derek Adams  fresh  114709              Time Out   \n",
       "2  Richard Corliss  fresh  114709         TIME Magazine   \n",
       "3      David Ansen  fresh  114709              Newsweek   \n",
       "4    Leonard Klady  fresh  114709               Variety   \n",
       "\n",
       "                                               quote review_date  rtid  \\\n",
       "0                                                NaN  2011-09-07  9559   \n",
       "1  So ingenious in concept, design and execution ...  2009-10-04  9559   \n",
       "2                  The year's most inventive comedy.  2008-08-31  9559   \n",
       "3  A winning animated feature that has something ...  2008-08-18  9559   \n",
       "4  The film sports a provocative and appealing st...  2008-06-09  9559   \n",
       "\n",
       "       title  \n",
       "0  Toy story  \n",
       "1  Toy story  \n",
       "2  Toy story  \n",
       "3  Toy story  \n",
       "4  Toy story  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- How many total reviews are in this data set?\n",
    "- How many unique critics?\n",
    "- How many movies were reviewed and rated \"fresh\", \"rotten\", or \"none\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 27631\n",
      "Unique critics: 690\n",
      "Number of movies :  2779\n"
     ]
    }
   ],
   "source": [
    "n_reviews = len(critics)\n",
    "n_critics = critics.critic.unique().size\n",
    "n_movies = critics.rtid.unique().size\n",
    "\n",
    "print(\"Total reviews: %i\" % n_reviews)\n",
    "print(\"Unique critics: %i\" % n_critics)\n",
    "print(\"Number of movies :  %i\" % n_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEeCAYAAABhd9n1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZ3/D522BDmIdoy6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x228dcf66a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = critics.groupby(\"fresh\").size().plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>True</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>True</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>True</td>\n",
       "      <td>114709</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            critic fresh    imdb    publication  \\\n",
       "1      Derek Adams  True  114709       Time Out   \n",
       "2  Richard Corliss  True  114709  TIME Magazine   \n",
       "3      David Ansen  True  114709       Newsweek   \n",
       "\n",
       "                                               quote review_date  rtid  \\\n",
       "1  So ingenious in concept, design and execution ...  2009-10-04  9559   \n",
       "2                  The year's most inventive comedy.  2008-08-31  9559   \n",
       "3  A winning animated feature that has something ...  2008-08-18  9559   \n",
       "\n",
       "       title  \n",
       "1  Toy story  \n",
       "2  Toy story  \n",
       "3  Toy story  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = critics.copy()\n",
    "df = df.dropna()\n",
    "df['fresh'] = df['fresh'] == 'fresh'\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>fresh</th>\n",
       "      <th>fresh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Roger Ebert</th>\n",
       "      <td>0.673145</td>\n",
       "      <td>1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>James Berardinelli</th>\n",
       "      <td>0.603234</td>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Janet Maslin</th>\n",
       "      <td>0.708738</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variety Staff</th>\n",
       "      <td>0.725581</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jonathan Rosenbaum</th>\n",
       "      <td>0.599515</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jessica Reaves</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nick Schager</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nicole Arthur</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nina Caplan</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harold C. Schonberg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mean   len\n",
       "                        fresh fresh\n",
       "critic                             \n",
       "Roger Ebert          0.673145  1132\n",
       "James Berardinelli   0.603234   804\n",
       "Janet Maslin         0.708738   515\n",
       "Variety Staff        0.725581   430\n",
       "Jonathan Rosenbaum   0.599515   412\n",
       "Jessica Reaves       1.000000     1\n",
       "Nick Schager         0.000000     1\n",
       "Nicole Arthur        0.000000     1\n",
       "Nina Caplan          1.000000     1\n",
       "Harold C. Schonberg  0.000000     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics_stats = pd.pivot_table(df,\n",
    "               index = ['critic'],\n",
    "               values = ['fresh'],\n",
    "               aggfunc=[np.mean,len],\n",
    "               fill_value=0).sort_values(('len','fresh'), ascending = False)\n",
    "critics_stats.head().append(critics_stats.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "### NLP Concepts\n",
    "- **Corpus:** All documents being analyzed. (plural: corpora)\n",
    "- **document:** A single element of the corpus\n",
    "- **Lexicon (or vocabulary):** All the words across all documents.\n",
    "- **Tokenization**: in each document, seperate each word into a element of a vector. Essential step for building TFM..\n",
    "- **Bag of words:** When vectorized, we no longer retain order.\n",
    "- **Term frequency Matrix**: A matrix of documents and words that belong. Each document occupies certain cells that belong to a word. Multiples of the same word show up as frequencies. Because this metric biases longer sentences, normalize with length of vector.\n",
    "- **Inverse Document Frequency Matrix**: Another frequency matrix that shows how frequently a word appears in other documents. \n",
    "- **Term Frequency - Inverse Document Frequency**: Combination of TF*IDF has properties of both such that:\n",
    "    - Words that show up frequently in a sentence are given higher weight\n",
    "    - Words that show up TOO frequently across all documents are given lower weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**corpus**\n",
    "\n",
    "`A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree. The grapes seemed ready to burst with juice, and the Fox's mouth watered as he gazed longingly at them.`\n",
    "\n",
    "**Lexicon** \n",
    "\n",
    "$V = \\left\\{\\right.$ `a, along, and, as, at, beautiful, branches, bunch, burst, day, fox, fox's, from, gazed, grapes, hanging, he, juice, longingly, mouth, of, one, ready, ripe, seemed, spied, the, them, to, trained, tree, vine, watered, with`$\\left.\\right\\}$\n",
    "\n",
    "**document**\n",
    "\n",
    "`A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree`\n",
    "\n",
    "** Tokenization **\n",
    "\n",
    "`[A, Fox, one, day, spied, a, beautiful, bunch, of, ripe, grapes, hanging, from, a, vine, trained, along, the, branches, of, a, tree]`\n",
    "\n",
    "**Count Vector: Relates the above document to the lexicon**\n",
    "\n",
    "$$\\bar V(d) = \\left( 4,1,0,0,0,1,1,1,0,1,1,0,1,0,1,1,0,0,0,0,2,1,0,1,0,0,1,0,0,0,1,1,0,0 \\right)$$\n",
    "\n",
    "or more succinctly as\n",
    "\n",
    "`[(0, 4), (1, 1), (5, 1), (6, 1), (7, 1), (9, 1), (10, 1), (12, 1), (14, 1), (15, 1), (20, 2), (21, 1), (23, 1),`\n",
    "`(26, 1), (30, 1), (31, 1)]`\n",
    "\n",
    "along with a dictionary\n",
    "\n",
    "``\n",
    "{\n",
    "    0: a, 1: along, 5: beautiful, 6: branches, 7: bunch, 9: day, 10: fox, 12: from, 14: grapes, \n",
    "    15: hanging, 19: mouth, 20: of, 21: one, 23: ripe, 24: seemed, 25: spied, 26: the, \n",
    "    30: tree, 31: vine, \n",
    "}\n",
    "``\n",
    "\n",
    "\n",
    "\n",
    "In our **rotten tomatoes data set**:\n",
    "- Together, all data in \"quote\" is considered our corpus\n",
    "- Each individual quote is a document\n",
    "- The lexicon: the unique words across all documents in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.quote\n",
    "y = df.fresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create \"Term-Frequency\" Matrix\n",
    "- Measures how often a word shows up as a proportion of all words in a document.\n",
    "    - TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "    - Denominator term normalizes so does not bias to longer sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14792x21799 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 259300 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "term_doc_matrix = vectorizer.fit_transform(X)\n",
    "term_doc_matrix ## this is an important object for new data coming in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore one review. \n",
    "- Remove all the '0' cases to get just counts of words\n",
    "- Change True to False to get Counts vs Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:1, Critic: Derek Adams\n",
      "\n",
      "So ingenious in concept, design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm.\n",
      "\n",
      "Frequency\n",
      "\n",
      "{0: {'and': 2, 'be': 1, 'by': 1, 'charm': 1, 'concept': 1, 'could': 1, 'design': 1, 'engulfed': 1, 'execution': 1, 'in': 1, 'ingenious': 1, 'it': 1, 'its': 1, 'on': 1, 'postage': 1, 'screen': 1, 'sized': 1, 'so': 1, 'stamp': 1, 'still': 1, 'that': 1, 'watch': 1, 'you': 1}}\n",
      "\n",
      "Frequency / Total words\n",
      "\n",
      "{0: {'and': 0.083333333333333329, 'be': 0.041666666666666664, 'by': 0.041666666666666664, 'charm': 0.041666666666666664, 'concept': 0.041666666666666664, 'could': 0.041666666666666664, 'design': 0.041666666666666664, 'engulfed': 0.041666666666666664, 'execution': 0.041666666666666664, 'in': 0.041666666666666664, 'ingenious': 0.041666666666666664, 'it': 0.041666666666666664, 'its': 0.041666666666666664, 'on': 0.041666666666666664, 'postage': 0.041666666666666664, 'screen': 0.041666666666666664, 'sized': 0.041666666666666664, 'so': 0.041666666666666664, 'stamp': 0.041666666666666664, 'still': 0.041666666666666664, 'that': 0.041666666666666664, 'watch': 0.041666666666666664, 'you': 0.041666666666666664}}\n"
     ]
    }
   ],
   "source": [
    "review_no = 0 \n",
    "\n",
    "review_1 = pd.DataFrame(term_doc_matrix.toarray())\n",
    "review_1 = review_1.iloc[[0]]\n",
    "review_1.columns = vectorizer.get_feature_names()\n",
    "\n",
    "review_1_cts = (\n",
    "    review_1.T[review_1.T[0]!= 0 ]\n",
    "    ).to_dict()\n",
    "\n",
    "review_1_norm = (\n",
    "    review_1.T[review_1.T[0]!= 0 ]/(\n",
    "    len(df.iloc[review_no][4].split()))\n",
    "    ).to_dict()\n",
    "\n",
    "print(\"Review:%i, Critic: %s\" % (review_no+1,df.iloc[review_no][0]))\n",
    "print(\"\")\n",
    "print(X[review_no+1])\n",
    "print(\"\")\n",
    "print(\"Frequency\")\n",
    "print(\"\")\n",
    "print(str(review_1_cts))\n",
    "print(\"\")\n",
    "print(\"Frequency / Total words\")\n",
    "print(\"\")\n",
    "print(str(review_1_norm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice now that none of the words are in order. This is our **\"bag of words\"**. We lose meaning, imagine \"cat in the hat\" vs \"hat in the cat\" = [cat,hat, in, the]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_doc_matrix_df = term_doc_matrix.todense()\n",
    "term_doc_matrix_df = pd.DataFrame(term_doc_matrix_df, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Inverse document Frequency**\n",
    "\n",
    "Measures how important a word is. In TF, \"And\" is of equal value to the word \"Inventive\". But in reality, \"And\" is a frequent term across all reviews.\n",
    "\n",
    "Other words of little importance may be: \"is\", \"of\", and \"that\". \n",
    "\n",
    "Thus our goal is to:\n",
    "- Give little weight to words of little importance (show up very frequently)\n",
    "- Give more weight to words that have high importance (Shows up rarely)\n",
    "- Apply this to our term frequency \n",
    "Equation is:\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0014</th>\n",
       "      <th>007</th>\n",
       "      <th>044</th>\n",
       "      <th>07</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>104</th>\n",
       "      <th>...</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zorros</th>\n",
       "      <th>zowie</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zzzzzs</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21799 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  0014  007  044   07   10  100  101  102  104    ...      zooming  \\\n",
       "0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...          0.0   \n",
       "1  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...          0.0   \n",
       "2  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...          0.0   \n",
       "3  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...          0.0   \n",
       "4  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...          0.0   \n",
       "\n",
       "   zooms  zorro  zorros  zowie  zucker  zweibel  zwick  zzzzzs  zzzzzzzzz  \n",
       "0    0.0    0.0     0.0    0.0     0.0      0.0    0.0     0.0        0.0  \n",
       "1    0.0    0.0     0.0    0.0     0.0      0.0    0.0     0.0        0.0  \n",
       "2    0.0    0.0     0.0    0.0     0.0      0.0    0.0     0.0        0.0  \n",
       "3    0.0    0.0     0.0    0.0     0.0      0.0    0.0     0.0        0.0  \n",
       "4    0.0    0.0     0.0    0.0     0.0      0.0    0.0     0.0        0.0  \n",
       "\n",
       "[5 rows x 21799 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(term_doc_matrix) # notice how this can be \"fit\", \"fit_transform\", vs \"transform\"\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns= vectorizer.get_feature_names())\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what if we wanted a vector representation of a new sentence?\n",
    "\n",
    "\"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A. tfm</th>\n",
       "      <th>B. tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>1</td>\n",
       "      <td>0.409039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1</td>\n",
       "      <td>0.341067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>1</td>\n",
       "      <td>0.400710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>1</td>\n",
       "      <td>0.413791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>1</td>\n",
       "      <td>0.409039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>over</th>\n",
       "      <td>1</td>\n",
       "      <td>0.252316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>1</td>\n",
       "      <td>0.362456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2</td>\n",
       "      <td>0.149059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       A. tfm  B. tfidf\n",
       "brown       1  0.409039\n",
       "dog         1  0.341067\n",
       "fox         1  0.400710\n",
       "jumps       1  0.413791\n",
       "lazy        1  0.409039\n",
       "over        1  0.252316\n",
       "quick       1  0.362456\n",
       "the         2  0.149059"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence = [\"The quick brown fox jumps over the lazy dog.\"]\n",
    "tfm_new = vectorizer.transform(new_sentence)\n",
    "tfidf_new = transformer.transform(tfm_new)\n",
    "\n",
    "new_sentence = pd.DataFrame({\"A. tfm\":tfm_new.toarray()[0],\"B. tfidf\":tfidf_new.toarray()[0]}, index = vectorizer.get_feature_names())\n",
    "new_sentence[new_sentence.iloc[:,1] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a \"Make X\" function\n",
    "that gives you:\n",
    "- New X Frame\n",
    "- Vectorizer to get TFM\n",
    "- Transformer that gives you TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_X(data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    term_doc_matrix = vectorizer.fit_transform(data)\n",
    "    term_doc_matrix = term_doc_matrix.todense()\n",
    "    term_doc_matrix = pd.DataFrame(term_doc_matrix)\n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf = transformer.fit_transform(term_doc_matrix)\n",
    "    tfidf = pd.DataFrame(tfidf.toarray())\n",
    "    return tfidf, vectorizer, transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training a Classifier\n",
    "At this point you might have employed the following during feature engineering:\n",
    "1. Dealt with contractions and abbreviations\n",
    "2. normalized your vocabulary via\n",
    "    - Case Norming\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "3. included a stop words list\n",
    "    - words to exclude such as \"it\" or \"the\" \n",
    "4. Parts of Speech Tagging\n",
    "5. Created \"N-Grams\" where your token looks like \n",
    "    - [\"Ice Cream\", \"New York City\"] rather than\n",
    "    - [New, york, city, ice, cream]\n",
    "6. Dimensionality reduction\n",
    "7. included meta data such as time and critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>True</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>True</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>True</td>\n",
       "      <td>114709</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>True</td>\n",
       "      <td>114709</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>True</td>\n",
       "      <td>114709</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic fresh    imdb     publication  \\\n",
       "1         Derek Adams  True  114709        Time Out   \n",
       "2     Richard Corliss  True  114709   TIME Magazine   \n",
       "3         David Ansen  True  114709        Newsweek   \n",
       "4       Leonard Klady  True  114709         Variety   \n",
       "5  Jonathan Rosenbaum  True  114709  Chicago Reader   \n",
       "\n",
       "                                               quote review_date  rtid  \\\n",
       "1  So ingenious in concept, design and execution ...  2009-10-04  9559   \n",
       "2                  The year's most inventive comedy.  2008-08-31  9559   \n",
       "3  A winning animated feature that has something ...  2008-08-18  9559   \n",
       "4  The film sports a provocative and appealing st...  2008-06-09  9559   \n",
       "5  An entertaining computer-generated, hyperreali...  2008-03-10  9559   \n",
       "\n",
       "       title  \n",
       "1  Toy story  \n",
       "2  Toy story  \n",
       "3  Toy story  \n",
       "4  Toy story  \n",
       "5  Toy story  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2,vectorizer_2, transformer_2 = make_X(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14792, 21799), (14792,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split\n",
    "- Set random seed to 100 for reproducibility\n",
    "- Split data to 85% train, 15% test.\n",
    "- Other methods include k-folds crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12573, 21799), (12573,), (2219, 21799), (2219,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(X2,y, test_size=0.15, random_state=100)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 0.818500\n",
      "Accuracy on Test Data: 0.715638\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on Training Data: %f\" % clf.score(X_train,y_train))\n",
    "print(\"Accuracy on Test Data: %f\" % clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.30      0.45       863\n",
      "       True       0.69      0.98      0.81      1356\n",
      "\n",
      "avg / total       0.77      0.72      0.67      2219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_pred = y_pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1180712 ,  0.27084272],\n",
       "       [ 0.0135196 ,  0.59756647]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_pred= y_pred, y_true=y_test)\n",
    "cm / cm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the strongly predictive features?\n",
    "\n",
    "We use a neat trick to identify strongly predictive features (i.e. words). \n",
    "\n",
    "* first, create a data set such that each row has exactly one feature. This is represented by the identity matrix.\n",
    "    - This represents the fact that each document will have exactly one word. One for each word that exists in our entire lexicon. \n",
    "* use the trained classifier to make predictions on this matrix\n",
    "* sort the rows by predicted probabilities, and pick the top and bottom $K$ rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(fresh | word)\n",
      "         masterpiece 0.96\n",
      "             delight 0.95\n",
      "          remarkable 0.94\n",
      "            touching 0.94\n",
      "            hypnotic 0.91\n",
      "           absorbing 0.91\n",
      "         beautifully 0.91\n",
      "        entertaining 0.91\n",
      "            intimate 0.91\n",
      "             rousing 0.91\n",
      "Bad words\t     P(fresh | word)\n",
      "            sluggish 0.18\n",
      "       disappointing 0.17\n",
      "               bland 0.17\n",
      "         forgettable 0.17\n",
      "       unfortunately 0.16\n",
      "           pointless 0.15\n",
      "      disappointment 0.14\n",
      "             unfunny 0.14\n",
      "          uninspired 0.13\n",
      "                lame 0.11\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(N=X_train.shape[1]) # create a matrix of N by N with \"Identity\" meaning diagonal is 1\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:10]]\n",
    "bad_words = words[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print(\"Good words\\t     P(fresh | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(fresh | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
