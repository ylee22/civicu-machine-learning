{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing IMDB Data in Keras using RNNs\n",
    "\n",
    "Let's use Recurrent Neural Networks, and in particular LSTMs, to perform sentiment analysis in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data\n",
    "\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at (vocabulary size). Let's set it at to 5000, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb  # import the built-in imdb dataset in Keras\n",
    "\n",
    "# Set the vocabulary size\n",
    "vocabulary_size = 5000\n",
    "\n",
    "# Load in training and test data (note the difference in convention compared to scikit-learn)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "print(\"Loaded dataset with {} training samples, {} test samples\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examining the data\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Review ---\n",
      "[1, 13, 244, 6, 194, 337, 6, 2, 787, 1716, 5, 207, 110, 98, 32, 5, 14, 9, 31, 7, 4, 118, 45, 163, 731, 5, 6, 356, 13, 386, 14, 18, 32, 2088, 45, 87, 18, 117, 362, 88, 45, 73, 2379, 5, 87, 18, 1473, 5, 1629, 88, 45, 163, 5, 24, 120, 4, 350, 13, 296, 12, 54, 13, 16, 117, 5, 13, 131, 106, 12, 150, 12, 47, 87, 411, 15, 61, 223, 5, 13, 3194, 32, 4, 58, 4, 116, 9, 87, 5, 12, 115, 214, 154, 48, 25, 40, 2447, 2953, 5, 2, 25, 80, 119, 14, 207, 296, 111, 6, 2379, 20, 11, 61, 58, 5, 14, 9, 4, 118, 7, 98, 32, 806, 910, 13, 545, 386, 14, 20, 5, 32, 4, 2, 787, 1716, 287, 36, 32, 1271, 8, 32, 2088, 5, 26, 32, 955, 5, 55, 441]\n",
      "--- Label ---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Print a sample review and its label\n",
    "idx = 2\n",
    "print(\"--- Review ---\")\n",
    "print(X_train[idx])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the label is an integer (0 for negative, 1 for positive), and the review itself is stored as a sequence of integers. These are word IDs that have been preassigned to individual words. To map them back to the original words, you can use the dictionary returned by imdb.get_word_index()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 6s 4us/step\n",
      "--- Review (with words) ---\n",
      "['the', 'was', 'rather', 'is', 'thought', 'completely', 'is', 'and', '9', 'pop', 'to', 'always', 'life', 'any', 'an', 'to', 'as', 'it', 'by', 'br', 'of', 'where', 'if', 'makes', 'falls', 'to', 'is', 'need', 'was', 'wonderful', 'as', 'but', 'an', 'logic', 'if', 'him', 'but', 'over', 'production', 'most', 'if', 'much', 'danger', 'to', 'him', 'but', 'fresh', 'to', 'listen', 'most', 'if', 'makes', 'to', 'his', 'show', 'of', 'try', 'was', 'plays', 'that', 'no', 'was', 'with', 'over', 'to', 'was', 'these', 'character', 'that', 'years', 'that', 'there', 'him', 'dialogue', 'for', 'only', 'whole', 'to', 'was', 'sons', 'an', 'of', 'my', 'of', 'love', 'it', 'him', 'to', 'that', 'best', 'role', 'work', 'what', 'have', 'just', 'texas', 'interpretation', 'to', 'and', 'have', 'into', 'did', 'as', 'always', 'plays', 'plot', 'is', 'danger', 'on', 'this', 'only', 'my', 'to', 'as', 'it', 'of', 'where', 'br', 'any', 'an', 'fall', 'badly', 'was', 'fight', 'wonderful', 'as', 'on', 'to', 'an', 'of', 'and', '9', 'pop', 'worth', 'from', 'an', 'attack', 'in', 'an', 'logic', 'to', 'he', 'an', 'plenty', 'to', 'time', 'overall']\n",
      "--- Label ---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Map word IDs back to words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print(\"--- Review (with words) ---\")\n",
    "print([id2word.get(i, \" \") for i in X_train[idx]])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the data\n",
    "\n",
    "In order to feed this data into your RNN, all input documents must have the same length. Let's limit the maximum review length to `max_words` by truncating longer reviews and padding shorter reviews with a null value (0). You can accomplish this easily using the `pad_sequences()` function in Keras. For now, set `max_words` to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-46f8f6ff74be>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-46f8f6ff74be>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    X_train = ?\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Set the maximum number of words per document (for both training and testing)\n",
    "max_words = 500\n",
    "\n",
    "# TODO: Pad sequences in X_train and X_test\n",
    "X_train = ?\n",
    "X_test = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Design an RNN model for sentiment analysis\n",
    "\n",
    "Build your model architecture in the code cell below. We have imported some layers from Keras that you might need but feel free to use any other layers / transformations you like.\n",
    "Remember that your input is a sequence of words (technically, integer word IDs) of maximum length = max_words, and your output is a binary sentiment label (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# TODO: Design your model\n",
    "model = Sequential()\n",
    "model.add\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the model\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile your model, specifying a loss function, optimizer, and metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once compiled, you can kick off the training process. There are two important training parameters that you have to specify - batch size and number of training epochs, which together with your model architecture determine the total training time.\n",
    "Training may take a while, so grab a cup of coffee, or better, go for a hike! If possible, consider using a GPU, as a single training run can take several hours on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify training parameters: batch size and number of epochs\n",
    "\n",
    "# TODO(optional): Reserve/specify some training data for validation (not to be used for training)\n",
    "\n",
    "# TODO: Train your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the model\n",
    "\n",
    "Once you have trained your model, it's time to see how well it performs on unseen test data.\n",
    "This will give you the accuracy of the model. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
    "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
